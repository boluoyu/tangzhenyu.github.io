---
layout: post
category: paper_notes
title: SGDR-STOCHASTIC GRADIENT DESCENT WITH WARM RESTARTS
date: 2017-08-11
---
# SGDR-STOCHASTIC GRADIENT DESCENT WITH WARM RESTARTS

Partial warm restarts are also gaining popularity in gradientbased optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions

The main difficulty in training a DNN is then associated with the scheduling of the learning rate and the amount of L2 weight decay regularization employed.

$v_t$ $u_t$

![](/assets/paper_notes/linknet/linknet1.jpg)

**RESTARTS IN GRADIENT-FREE OPTIMIZATION**


**RESTARTS IN GRADIENT-BASED OPTIMIZATION**


# Referrences:
- Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and Kilian Q. Weinberger.Snapshot ensembles: Train 1, get m for free. ICLR 2017 submission, 2016a.

