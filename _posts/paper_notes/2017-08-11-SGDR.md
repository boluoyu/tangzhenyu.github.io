---
layout: post
category: paper_notes
title: SGDR-STOCHASTIC GRADIENT DESCENT WITH WARM RESTARTS
date: 2017-08-11
---
# SGDR-STOCHASTIC GRADIENT DESCENT WITH WARM RESTARTS

Partial warm restarts are also gaining popularity in gradientbased optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions

The main difficulty in training a DNN is then associated with the scheduling of the learning rate and the amount of L2 weight decay regularization employed.

$v_t$ $u_t$

![](/assets/paper_notes/sgdr/sgdr1.jpg)

In this paper, proposed a approach to periodically simulate warm restarts of SGD, where in each restart the
learning rate is initialized to some value and is scheduled to decrease. Four different instantiations
of this new learning rate schedule are visualized in Figure 1. 

Main contributions:
- Empirical results suggest that SGD with warm restarts requires 2× to 4× fewer epochs than the currently-used learning rate schedule schemes to achieve comparable or even better results.
- Combining the networks obtained right before restarts in an ensemble following the approach proposed by Huang et al. (2016a) improves our results further to 3.14% for CIFAR-10 and 16.21% for CIFAR-10

**RESTARTS IN GRADIENT-FREE OPTIMIZATION**
Gradient-free optimization approaches based on niching methods (Preuss, 2015) usually can deal with task(optimizing multimodal functions one may want to find all global and local optima) by covering the search space with dynamically allocated niches of local optimizers

**RESTARTS IN GRADIENT-BASED OPTIMIZATION**
Gradient-based optimization algorithms such as BFGS can also perform restarts to deal with multimodal functions

# STOCHASTIC GRADIENT DESCENT WITH WARM RESTARTS (SGDR)
Take into account the stochastiscity,the existing restart techniquescan also be used for SGD.Considering averaged gradients and losses per epoch.

A new warmstarted run/restart of SGD once Ti epochs are performed, where i is the index of the run. Importantly, the restarts are not performed from scratch but emulated by increasing the learning rate ηt while the old value of xt is used as an initial solution

Within the i-th run, we decay the learning rate with a cosine annealing for each batch as follows:

![](/assets/paper_notes/sgdr/sgdr2.jpg)

# Referrences:
- Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and Kilian Q. Weinberger.Snapshot ensembles: Train 1, get m for free. ICLR 2017 submission, 2016a.

